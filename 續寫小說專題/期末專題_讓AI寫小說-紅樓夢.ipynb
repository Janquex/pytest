{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算機程式期末專案\n",
    "### 專案名稱：【讓AI寫小說-紅樓夢】\n",
    "#### ● 成員：105302014金融四-蔡孟純 / 107308049風管二-廖品彥 / 108701025應數一-李邵廷 / 108701026應數一-張雯玲\n",
    "#### ● 動機發想：平時我們在看小說時，有時會遇到爛尾或是結尾的狀況。為了彌補這些殘念，於是本次期末專案我們將運用學期所學訓練AI幫我們寫寫小說!\n",
    "#### ● 資料來源：期中時原訂以《後宮甄嬛傳》做為文本。然而，考量到資料著作權的公開合法性，經組內商量後決定換成《紅樓夢》。已從網路上取得公開資料，紅樓夢全套TXT檔案，共120回，總計88萬字。(資料網址：http://www.speedy7.com/cn/stguru/cht/redmansions.htm)\n",
    "#### ● 實作方法：共分為7個步驟(細節請至各步驟詳看)\n",
    "#### Step1.定義問題及要解決的任務\n",
    "#### Step2.準備原始數據、資料清洗\n",
    "#### Step3.建立能丟入模型的資料集\n",
    "#### Step4.定義能解決問題的函式集\n",
    "#### Step5.定義評量函式好壞的指標\n",
    "#### Step6.訓練並選擇出最好的函式 \n",
    "#### Step7.將函式 / 模型拿來做預測\n",
    "\n",
    "#### 專案分工：\n",
    "#### 我們大致將程式碼分為四個大主題，分工上要求每人選擇一個主題來消化，並在進度周負責教授予其他組員。\n",
    "#### 1廖品彥：Step2. 研究如何將每個文字以字典的方式儲存(Tokenizer)\n",
    "#### 2李邵廷：Step3. 研究如何做出輸出、輸入資料集\n",
    "#### 3張雯玲：Step4.研究如何建立模型並了解損失函數與優化器的功能\n",
    "#### 4蔡孟純：Step6. 研究如何改變參數達到調整冷門字出現的比率\n",
    "\n",
    "#### ● 結果呈現：做完模型之後，我們十分好奇「訓練次數」、「溫度高低」對於文本生成的影響為何？因此我們將分為兩部分比較：\n",
    "\n",
    "#### （預設：起始字為第87號字「鳳」，生成1000字文本）\n",
    "#### 1. 溫度均設為0.5，訓練次數分別為1次/10次/50次\n",
    "#### 2. 訓練次數皆設為10，溫度分別設為0.1/0.5/2\n",
    "#### ● 操作說明：我們總共訓練出三個模型，分別為1/10/50次，模型資料均放在下雲端連結資料夾中。模型中的文本起始字、生成長度以及溫度均可直接在程式碼標註處更改，以生成不同文本。\n",
    "#### （備註：需下載附件中的紅樓夢txt檔案，以及各模型訓練權重，並載入模型後才可操作）\n",
    "\n",
    "\n",
    "#### ● 結論：根據前面的文本生成結果，我們可以得到以下結論：\n",
    "#### 1.溫度相同的情況下，訓練次數越多越好\n",
    "#### 2.訓練次數相同的情況下，溫度設在0.5左右的效果最好\n",
    "#### ● 實作心得：\n",
    "#### (1) 蔡：身為接觸python的新手，在此次專案合作上最大的感觸莫過於3點。分別是：「不會就多看，多數情況下Google可以救你」、「不要妄想一步登天，一步步的老實爬吧…」以及「不要忘了分工！大家特長不一樣」。最後，感謝組員們配合以及老師&助教諮詢協助，使此次專案能順利產出\n",
    "#### (2) 廖：非常慶幸自己有修炎龍老師開的這堂課，從大一時便開始對深度學習產生興趣，從此次專案學習到3點，分別是，深度學習的知識及應用、如何與不同背景的同學溝通合作，最後是要問正確的問題是個至關正要的能力!!! 最後，感謝組員的相互配合及導師助教的悉心指導。\n",
    "#### (3) 張 ：在專案討論跟實作的過程中發現自己有蠻多盲點跟不太懂的地方，不過經過組內大家互相分工、研究程式碼、分享，加上自己要好好利用網路搜尋資料及開口問，雖然會花比較久的時間，不過這樣可以一步一腳印地去理解問題跟解決問題，也非常感謝同組的夥伴、老師和助教，讓我在專案和課堂中都學到了很多！\n",
    "#### (4) 李：在學期初時，從來沒有想過自己能夠參與如此一個有趣的主題，對我來說，”用AI寫小說”專案是一個非常難忘的經驗、很開心這次和組員一起合作完成專案、也感謝助教的指導。雖然小組每週有排程與規劃時間進行專案，但是並不是每個階段都能順利達成，這時，上網多方查尋資料和後續的討論就是一大關鍵了，因為每個組員發現的重點都不一樣，相互交流後便能迸出新的視野、讓整個專案一步一步地向前推進。\n",
    "\n",
    "\n",
    "\n",
    "#### —————————————————————————————————————————\n",
    "#### ————————————————程式碼如下———————————————————\n",
    "#### —————————————————————————————————————————\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.定義問題及要解決的任務：\n",
    "#### (1)找出一個紅樓夢的語言模型，讓該模型在被餵進一段文字以後，能吐出類似紅樓夢的文章。\n",
    "#### (2)模型為給定一段文字的單位(字或詞)序列後，可以吐出下一個合理的文字單位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline                \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.準備原始數據、資料清洗\n",
    "#### (1)上網下載紅樓夢的txt檔\n",
    "#### (2)將文檔內部廣告資料清洗\n",
    "#### (3)將文檔存入字串\n",
    "#### (4)將字串分割辨識成不同的字與詞並存入對應字典內"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=[]\n",
    "with open('C:/Users/ACER/pytest/deep learning/續寫小說專題/紅樓夢Big5.txt', 'r', encoding='MS950') as f:   # encoding是文字編碼，這次的是MS950\n",
    "    for chunk in iter(lambda: f.read(500), ''):     # 跑回圈， 每次讀入500個字直到文本讀完為止\n",
    "        text.append(chunk)                          # chunk是變數，只是任意名稱，可以隨意替代\n",
    "    text = ''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "紅樓夢小說共有 887684 中文字\n",
      "包含了 4523 個獨一無二的字\n"
     ]
    }
   ],
   "source": [
    "n = len(text)\n",
    "w = len(set(text))\n",
    "print(f\"紅樓夢小說共有 {n} 中文字\")\n",
    "print(f\"包含了 {w} 個獨一無二的字\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer # import Tokenizer 模組。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 4600 #以4600字作為字數上限，方便後續Token的處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = num_words) \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True,filters='') # 讓 tokenizer 讀過紅樓夢全文，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將每個新出現的字加入字典並將中文字轉成對應的數字索引\n",
    "tokenizer.fit_on_texts(text) # 讀全文\n",
    "text_as_int = tokenizer.texts_to_sequences([text])[0] #建立字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3. 建立能丟入模型的資料集\n",
    "#### (1)輸入、輸出的數據格式皆是一個固定長度(可自行定義，以8個字舉例↓)的數字序列，而後者是前者往左位移一個數字的結果。\n",
    "#### (2)舉例：賈夫人仙逝揚州城>>>>夫人仙逝揚州城"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原本的中文字序列：\n",
      "\n",
      "['高', '低', '不', '擇', '，', '還', '說', '『', '通', '靈', '』', '不', '『', '通', '靈', '』']\n",
      "\n",
      "--------------------\n",
      "\n",
      "轉換後的索引序列：\n",
      "\n",
      "[424, 746, 4, 1263, 1, 67, 14, 146, 575, 565, 144, 4, 146, 575, 565, 144]\n",
      "--------------------\n",
      "\n",
      "左移後的索引序列：\n",
      "\n",
      "[746, 4, 1263, 1, 67, 14, 146, 575, 565, 144, 4, 146, 575, 565, 144]\n",
      "\n",
      "文本序列：\n",
      "\n",
      "['低', '不', '擇', '，', '還', '說', '『', '通', '靈', '』', '不', '『', '通', '靈', '』']\n"
     ]
    }
   ],
   "source": [
    "# 隨機選取一個片段文本檢視Token處理結果，從21004到21020。\n",
    "s_idx = 21004\n",
    "e_idx = 21020\n",
    "partial_indices = text_as_int[s_idx:e_idx]\n",
    "partial_texts = [tokenizer.index_word[idx] for idx in partial_indices]\n",
    "\n",
    "# 範例（示範文字轉成字典）\n",
    "print(\"原本的中文字序列：\")\n",
    "print()\n",
    "print(partial_texts)\n",
    "print()\n",
    "print(\"-\" * 20)\n",
    "print()\n",
    "print(\"轉換後的索引序列：\")\n",
    "print()\n",
    "print(partial_indices)\n",
    "\n",
    "# 因為使用RNN預測，所以我們的訓練資料需要向左移一個字的結果\n",
    "# 可以和上面的顯示做對照，會發現全部左移一個字\n",
    "print(\"-\" * 20)\n",
    "print()\n",
    "print(\"左移後的索引序列：\")\n",
    "print() \n",
    "print(partial_indices[1:])\n",
    "print()\n",
    "print(\"文本序列：\")\n",
    "print()\n",
    "print(partial_texts[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 實際上我們會用更大的值來讓模型從更長的序列預測下個中文字\n",
    "SEQ_LENGTH = 10  # 數字序列長度\n",
    "BATCH_SIZE = 128 # 幾筆成對輸入/輸出\n",
    "\n",
    "# text_as_int 是一個 python list\n",
    "# 我們利用 from_tensor_slices 將其轉變成 TensorFlow 的 Tensor \n",
    "characters = tf\\\n",
    "    .data\\\n",
    "    .Dataset\\\n",
    "    .from_tensor_slices(\n",
    "        text_as_int)\n",
    "\n",
    "# 將被以數字序列表示的紅樓夢文本，拆成多個長度為 SEQ_LENGTH(10)的序列\n",
    "# 並將最後長度不滿 SEQ_LENGTH 的序列捨去\n",
    "sequences = characters\\\n",
    "    .batch(SEQ_LENGTH + 1,  # 實際上是用10+1個在跑\n",
    "           drop_remainder=True)\n",
    "\n",
    "# 紅樓夢全文所包含的成對輸入/輸出的數量\n",
    "# 商,代表有多少成對的資料\n",
    "steps_per_epoch = len(text_as_int) // SEQ_LENGTH\n",
    "\n",
    "# 這個函式專門負責把一個序列拆成兩個序列，分別代表輸入與輸出\n",
    "# （下段有 vis 解釋這在做什麼）\n",
    "def build_seq_pairs(chunk):\n",
    "    input_text = chunk[:-1]  #右移(輸入)\n",
    "    target_text = chunk[1:]  #左移(輸出)\n",
    "    return input_text, target_text\n",
    "\n",
    "# 將每個從文本擷取出來的序列，拆成兩個數字序列作為輸入／輸出序列\n",
    "# 再將得到的所有數據隨機打亂順序\n",
    "# 最後再一次拿出 BATCH_SIZE（128）筆數據，作為模型一次訓練步驟的所使用的資料\n",
    "# 將每個從文本擷取出來的序列套用上面定義的函式，拆成兩個數字序列作為輸入／輸出序\n",
    "# 洗牌，隨機訓練\n",
    "ds = sequences.map(build_seq_pairs).shuffle(steps_per_epoch).batch(BATCH_SIZE,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定義能解決問題的函式集\n",
    "#### (1)選擇RNN的LSTM模型\n",
    "#### i.先建立詞遷入層，將每個索引數字對應到一個高維空間的向量\n",
    "#### ii.在建立LSTM層，將序列數據依序讀入並做處理\n",
    "#### iii.加入Dense層，負責model每個中文字出現的可能性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 512)          2355200   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (128, None, 1024)         6295552   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 4600)         4715000   \n",
      "=================================================================\n",
      "Total params: 13,365,752\n",
      "Trainable params: 13,365,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "EMBEDDING_DIM = 512\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# 使用 keras 建立一個非常簡單的 LSTM 模型\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# 詞嵌入層\n",
    "# 將每個索引數字對應到一個高維空間的向量\n",
    "# input_dim : 有多少獨一無二的字，他會是一個矩陣，理論上來說我們要輸入4523 (可是我們輸入5000發現可以跑了XD)\n",
    "# output_dim : 降維\n",
    "# batch_input_shape :\n",
    "model.add(\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        batch_input_shape=[\n",
    "            BATCH_SIZE, None]    # None:虛無，不用定義\n",
    "))\n",
    "\n",
    "# LSTM 層\n",
    "# 負責將序列數據依序讀入並做處理\n",
    "model.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "    units=RNN_UNITS, \n",
    "    return_sequences=True,   # True: 每做一次都會返回，不打則是False\n",
    "    stateful=True,           # 記得他有返回\n",
    "    recurrent_initializer='glorot_uniform'  # 線性轉換 \n",
    "))\n",
    "\n",
    "# 全連接層\n",
    "# 負責 model 每個中文字出現的可能性\n",
    "# Dense : 神經元\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(   \n",
    "        num_words))    # 被壓制到5000\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定義評量函式好壞的指標\n",
    "#### (1)定義Loss Function : sparse_categorical_crossentropy\n",
    "#### (2)使用adam作為optimizer最小化損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數，決定模型一次要更新的步伐有多大\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# 定義模型預測結果跟正確解答之間的差異\n",
    "# 因為全連接層沒使用 activation func\n",
    "# from_logits= True \n",
    "def loss(y_true, y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred, from_logits=True)\n",
    "\n",
    "# 編譯模型，使用 Adam Optimizer 來最小化\n",
    "# 剛剛定義的損失函數\n",
    "# optimizer : 梯度降維，像解一個數學題找最佳解\n",
    "# learning_rate : 希望讓圖形跑道最合適的點\n",
    "model.compile(\n",
    "    optimizer=tf.keras\\\n",
    "        .optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE), #optimizer : 梯度降維，像解一個數學題找最佳解\n",
    "    loss=loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 訓練並選擇出最好的函式\n",
    "#### (1)定義epoch數量，並觀測結果狀況\n",
    "#### (2) 訓練完模型後，將權重儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 630 steps\n",
      "Epoch 1/50\n",
      "630/630 [==============================] - 502s 797ms/step - loss: 4.9594\n",
      "Epoch 2/50\n",
      "630/630 [==============================] - 510s 810ms/step - loss: 4.2971\n",
      "Epoch 3/50\n",
      "630/630 [==============================] - 518s 822ms/step - loss: 4.0707\n",
      "Epoch 4/50\n",
      "630/630 [==============================] - 533s 846ms/step - loss: 3.9049\n",
      "Epoch 5/50\n",
      "630/630 [==============================] - 540s 857ms/step - loss: 3.7614\n",
      "Epoch 6/50\n",
      "630/630 [==============================] - 553s 878ms/step - loss: 3.6329\n",
      "Epoch 7/50\n",
      "630/630 [==============================] - 555s 880ms/step - loss: 3.5135\n",
      "Epoch 8/50\n",
      "630/630 [==============================] - 560s 890ms/step - loss: 3.4035\n",
      "Epoch 9/50\n",
      "630/630 [==============================] - 560s 888ms/step - loss: 3.3003\n",
      "Epoch 10/50\n",
      "630/630 [==============================] - 559s 888ms/step - loss: 3.2056\n",
      "Epoch 11/50\n",
      "630/630 [==============================] - 558s 885ms/step - loss: 3.1181\n",
      "Epoch 12/50\n",
      "630/630 [==============================] - 559s 887ms/step - loss: 3.0350\n",
      "Epoch 13/50\n",
      "630/630 [==============================] - 576s 914ms/step - loss: 2.9593\n",
      "Epoch 14/50\n",
      "630/630 [==============================] - 563s 894ms/step - loss: 2.8866\n",
      "Epoch 15/50\n",
      "630/630 [==============================] - 565s 897ms/step - loss: 2.8198\n",
      "Epoch 16/50\n",
      "630/630 [==============================] - 568s 901ms/step - loss: 2.7574\n",
      "Epoch 17/50\n",
      "630/630 [==============================] - 571s 906ms/step - loss: 2.6986\n",
      "Epoch 18/50\n",
      "630/630 [==============================] - 570s 904ms/step - loss: 2.6445\n",
      "Epoch 19/50\n",
      "630/630 [==============================] - 572s 908ms/step - loss: 2.5939\n",
      "Epoch 20/50\n",
      "630/630 [==============================] - 568s 902ms/step - loss: 2.5453\n",
      "Epoch 21/50\n",
      "630/630 [==============================] - 569s 904ms/step - loss: 2.5004\n",
      "Epoch 22/50\n",
      "630/630 [==============================] - 566s 898ms/step - loss: 2.4575\n",
      "Epoch 23/50\n",
      "630/630 [==============================] - 563s 894ms/step - loss: 2.4167\n",
      "Epoch 24/50\n",
      "630/630 [==============================] - 561s 891ms/step - loss: 2.3826\n",
      "Epoch 25/50\n",
      "630/630 [==============================] - 562s 891ms/step - loss: 2.3449\n",
      "Epoch 26/50\n",
      "630/630 [==============================] - 559s 887ms/step - loss: 2.3115\n",
      "Epoch 27/50\n",
      "630/630 [==============================] - 552s 877ms/step - loss: 2.2788\n",
      "Epoch 28/50\n",
      "630/630 [==============================] - 563s 894ms/step - loss: 2.2507\n",
      "Epoch 29/50\n",
      "630/630 [==============================] - 558s 885ms/step - loss: 2.2242\n",
      "Epoch 30/50\n",
      "630/630 [==============================] - 554s 880ms/step - loss: 2.1954\n",
      "Epoch 31/50\n",
      "630/630 [==============================] - 563s 893ms/step - loss: 2.1697\n",
      "Epoch 32/50\n",
      "630/630 [==============================] - 563s 894ms/step - loss: 2.1467\n",
      "Epoch 33/50\n",
      "630/630 [==============================] - 565s 897ms/step - loss: 2.1243\n",
      "Epoch 34/50\n",
      "630/630 [==============================] - 557s 884ms/step - loss: 2.1016\n",
      "Epoch 35/50\n",
      "630/630 [==============================] - 562s 892ms/step - loss: 2.0820\n",
      "Epoch 36/50\n",
      "630/630 [==============================] - 552s 877ms/step - loss: 2.0632\n",
      "Epoch 37/50\n",
      "630/630 [==============================] - 563s 894ms/step - loss: 2.0432\n",
      "Epoch 38/50\n",
      "630/630 [==============================] - 562s 892ms/step - loss: 2.0239\n",
      "Epoch 39/50\n",
      "630/630 [==============================] - 563s 893ms/step - loss: 2.0097\n",
      "Epoch 40/50\n",
      "630/630 [==============================] - 562s 892ms/step - loss: 1.9911\n",
      "Epoch 41/50\n",
      "630/630 [==============================] - 564s 895ms/step - loss: 1.9738\n",
      "Epoch 42/50\n",
      "630/630 [==============================] - 559s 887ms/step - loss: 1.9622\n",
      "Epoch 43/50\n",
      "630/630 [==============================] - 555s 882ms/step - loss: 1.9444\n",
      "Epoch 44/50\n",
      "630/630 [==============================] - 562s 893ms/step - loss: 1.9307\n",
      "Epoch 45/50\n",
      "630/630 [==============================] - 563s 894ms/step - loss: 1.9204\n",
      "Epoch 46/50\n",
      "630/630 [==============================] - 564s 895ms/step - loss: 1.9035\n",
      "Epoch 47/50\n",
      "630/630 [==============================] - 558s 886ms/step - loss: 1.8938\n",
      "Epoch 48/50\n",
      "630/630 [==============================] - 562s 892ms/step - loss: 1.8805\n",
      "Epoch 49/50\n",
      "630/630 [==============================] - 563s 894ms/step - loss: 1.8694\n",
      "Epoch 50/50\n",
      "630/630 [==============================] - 560s 888ms/step - loss: 1.8600\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50 # 決定看幾篇紅樓夢文本\n",
    "history = model.fit(\n",
    "    ds, # 前面使用 tf.data 建構的資料集\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "model.save_weights('50_weight.ckpt') # 存取訓練權重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 將函式 / 模型拿來做預測\n",
    "#### (1) 重新架構一個模型，並匯入訓練權重\n",
    "#### (2) 調整溫度，並產出文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文章的模型與訓練的模型Batch不同，所以必須要重新架構一個模型，並匯入訓練權重\n",
    "\n",
    "# 只差在 BATCH_SIZE 為 1\n",
    "EMBEDDING_DIM = 512\n",
    "RNN_UNITS = 1024\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# 專門用來做生成的模型\n",
    "infer_model = tf.keras.Sequential()\n",
    "\n",
    "# 詞嵌入層\n",
    "infer_model.add(\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=num_words, \n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        batch_input_shape=[\n",
    "            BATCH_SIZE, None]\n",
    "))\n",
    "\n",
    "# LSTM 層\n",
    "infer_model.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "    units=RNN_UNITS, \n",
    "    return_sequences=True, \n",
    "    stateful=True\n",
    "))\n",
    "\n",
    "# 全連接層\n",
    "infer_model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        num_words))\n",
    "\n",
    "\n",
    "\n",
    "# 讀入之前訓練時儲存下來的權重\n",
    "infer_model.load_weights('50_weight.ckpt')\n",
    "infer_model.build(\n",
    "    tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#透過溫度調整字詞選取的隨機性\n",
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立一個生產文本的函式\n",
    "def generate_text(start_string, text_length):\n",
    "    seed_indices = [start_string]\n",
    "    text_generated = []\n",
    "    # 增加 batch 維度丟入模型取得預測結果後\n",
    "    # 再度降維，拿掉 batch 維度\n",
    "    input = tf.expand_dims(\n",
    "        seed_indices, axis=0)\n",
    "    selected_word = tokenizer.index_word[seed_indices[0]]\n",
    "    print(\"您輸入的字為「\", selected_word,\"」\", sep='')\n",
    "    print()\n",
    "    print('-'*20)\n",
    "    print()\n",
    "    predictions = infer_model(input)  \n",
    "    predictions = tf.squeeze(\n",
    "        predictions, 0)\n",
    "\n",
    "    predictions /= temperature\n",
    "    sampled_indices = tf.random\\\n",
    "        .categorical(\n",
    "            predictions, num_samples=1)\n",
    "    \n",
    "    input = tf.expand_dims(\n",
    "    sampled_indices, axis=0)\n",
    "    text_generated.append(tokenizer.index_word[int(sampled_indices)])\n",
    "    \n",
    "    # 設計迴圈產出文本，每次迴圈將產出一個字，並儲存變數，下次迴圈將以剛才的字做預測\n",
    "    for i in range(text_length):\n",
    "        predictions = infer_model(list(input)) #所需資料行代為串列，初次資料類型與迴圈資料類型不同，故拆開來處理\n",
    "        predictions = tf.squeeze(\n",
    "            predictions, 0)\n",
    "\n",
    "        predictions /= temperature\n",
    "        sampled_indices = tf.random\\\n",
    "            .categorical(\n",
    "                predictions, num_samples=1)\n",
    "        input = tf.expand_dims(\n",
    "        sampled_indices, axis=0)\n",
    "        text_generated.append(tokenizer.index_word[int(sampled_indices)])\n",
    "    text_generated =''.join(text_generated)\n",
    "    print(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您輸入的字為「鳳」\n",
      "\n",
      "--------------------\n",
      "\n",
      "姐兒和奴才要錢，才吃過飯來，一時又不見了，便不禁心。」那賈蓉又道：「你怎麼不來了？」賈母道：「我也知道，等我告訴她，她倒不是嫌人家骯髒，只管來，先從顯的是小孩子的東西，賞了我再找去。」說得眾人都笑了。賈母因說：「姨太太既不管這事，只是心裡知道，也不敢說。賈璉道：「你見過三姑娘那裡去，看她明兒見他在這裡蹲著弄水的，也有薦僧道的，既說仙去，又不好叫她回去了。」寶玉道：「你知道芹先時候送來的。」寶玉笑道：「你又說這個是誰？」一遍，三人各自去了。\n",
      "\n",
      "　　這裡黛玉添了何敢辯，遂一夜不曾合著樂樂，有何趣味。」說著，又命人撤去酒席，擺上飯來，自己便往後走來，只見襲人走來，說道：「你老人家說得很是。我們也不怕你，也不知道是什麼事？」賈璉道：「我們這原是隨便的叫了幾句，不覺心動神搖，要知道。」又命琥珀：「拿出手來替她拭淚。王夫人又問：「你怎麼不去？你不必和太太告訴我去，好就好，碰得不好意思告訴我，我還要求你外頭新書的話，又說：「綺兒也不大會作詩，看著我們睡了覺，空兒就把這一件烏雲豹的氅衣給他拭淚，笑道：「這是哪裡來的？」鳳姐道：「你們不好，你我不管這樣的人家，不但不能報效，何苦來，也就不提了。\n",
      "\n",
      "　　賈母因問：「這樣才好。」薛姨媽又說：「你們那裡知道這是四姑奶奶的造化了。只是不肯。」寶釵道：「我說你們放心，等我告訴你。」說著又向紅玉笑道：「我說你們這幾天總沒有這麼大福氣呢。」寶玉道：「我也不知道。」寶玉道：「我不信，我摘了十來個姑娘，也不相勸，只是怕我難纏的，將來難道你就不問我，我也不便告訴你們，又不知怎麼樣了，還聽見說，櫳翠庵的妙師父怎麼不會說話，只管望你璉二嬸子那裡要去，你趁早兒去。」寶玉道：「我們又不大會詩，別人也不用這些，所以我才到了那日，又不是客，讓我代祝：你若說不出來，正好衣，一面笑道：「這話極是。」\n",
      "\n",
      "　　賈母聽了，忙收了盒子，倚靠背，必要將你想，一面忙回身搶住，笑道：「這是怎麼說，若是不治，只怕老太太懸心。明日飯後，賈母叫她坐下，便問：「你既知道了，又說：「你們不必論叔姪，只論弟兄朋賀，生得形容秀美，第七日，賈母死後只和眾丫頭們便說：「有趣，有趣，倒說我們昧下。那個人叫做什麼？」寶玉聽了，心下便知。」寶玉忙道：「若是他們演習。他既不好，你怎麼樣？你道：「我說呢，你可知道？」寶玉道：「我知道你這是為何？」寶玉道：「我不信，說不得要討你們的關係了我，我就叫她來。」寶玉道：「\n"
     ]
    }
   ],
   "source": [
    "# 透過選取token的編碼，使模型開始預測並產出文本\n",
    "generate_text(87,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
